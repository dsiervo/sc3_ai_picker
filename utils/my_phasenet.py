# -*- coding: utf-8 -*-
"""My_PhaseNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rxYYbEOtxYB_1GdQjQfP_u6uYVMNPc-T
"""

import tensorflow as tf
from tensorflow.keras.layers import (Conv2DTranspose, Lambda, Permute, Dropout, concatenate, Reshape)

import os
keras = tf.keras


keras.backend.clear_session()  # For easy reset of notebook state.

"""Definimos una función para efectuar deconvolución en datos de una dimensión:"""

class PhaseNet:

  def __init__(self, trace_size,
               loss = 'cross_entropy',
               lstm = False):
    
    self.trace_size = trace_size
    self.inputs = keras.Input(shape=(self.trace_size,1,3))
    self.lstm = lstm


    """Definimos las arquitecturas principales capas de la red"""

    self.size = 7
    self.stride = 4
    self.init = 'he_normal'
    self.loss = loss
    self.activation = 'relu'

  def conv_relu(self, num_filters, name, bias=False, pad='same'): 
    return keras.layers.Conv2D(filters=num_filters,
                                use_bias=bias,
                                kernel_size=(self.size,1), 
                                padding=pad,
                                name=name,
                                activation=self.activation,
                                kernel_initializer=self.init)

  def conv_stride_relu(self, num_filters, name, bias=False, pad='same'):
    return keras.layers.Conv2D(filters=num_filters,
                                use_bias=bias,                               
                                kernel_size=(self.size,1), 
                                strides=(self.stride,1),
                                padding=pad,
                                name=name,
                                activation=self.activation,
                                kernel_initializer=self.init)

  def deconv_relu(self, num_filters, name, bias=False, pad='same'):

      return Conv2DTranspose(filters=num_filters,
                          use_bias=bias,                             
                          kernel_size=(self.size,1),
                          strides=(self.stride,1),
                          padding=pad,
                          name=name,
                          activation=self.activation,
                          kernel_initializer=self.init)

  def slice_and_concat(self, u, c):

    diff = u.shape[1] - c.shape[1]

    #print('u:', u.shape, 'c:', c.shape)
    # si la diferencia en cuentas es mayor a cero debe recortarce la red
    # convolucional transpuesta para que tenga las mismas dimenciones de la otra
    if diff != 0: 
      conc = concatenate([u[:,:-diff,:], c])
    else:
      conc = concatenate([u, c])
    
    return conc

  """Creamos el modelo utilizando las capas definidas en el paso anterior.
  ### Super capa input
  Está conformada por:
  * conv + relu --> No aumenta el número de filtros, mantiene dimensión del resto del tensor

  ### Super capas down
  Cada super capa down (excepto la 4) está conformada por:
  * conv + relu --> Aumenta el número de filtros (el doble), mantiene dimensión del resto del tensor
  * conv + stride + relu --> mantiene el número de filtros (canales), reduce a la cuarta parte la dimensión del resto del tensor

  La super capa **down_4** no lleva la capa del stride
  """

  def create_model(self):
    # input -> 8
    c1 = self.conv_relu(8, bias=True, name='input_conv')(self.inputs)
    # 0 down -> 8
    c1 = self.conv_relu(8, name='Down_0_inp')(c1) 

    ## 
    c2 = self.conv_stride_relu(8, name='Down_0_out')(c1) # -> 8
    # 1 down -> 8
    c2 = self.conv_relu(16, name='Down_1_inp')(c2)

    ## 
    c3 = self.conv_stride_relu(16, name='Down_1_out')(c2) # -> 16
    # 2 down -> 16
    c3 = self.conv_relu(32, name='Down_2_inp')(c3)

    ##
    c4 = self.conv_stride_relu(32, 'Down_2_out')(c3)  # -> 32
    # 3 down -> 32
    c4 = self.conv_relu(64, name='Down_3_inp')(c4)

    ## 
    c5 = self.conv_stride_relu(64, 'Down_3_out')(c4)  # -> 64
    #print(c5.shape)

    if not self.lstm:
      # 4 down -> 64
      c5 = self.conv_relu(128, name='Down_4')(c5) # -> 128

    else:
    # adding 5th dimension
      c5 = tf.keras.layers.Reshape(target_shape=(1,12,1,64))(c5)
      c5 = tf.keras.layers.ConvLSTM2D(
                                  filters=128,
                                  kernel_size=(self.size,1), 
                                  name='ConvLSTM',
                                  padding='same',
                                  recurrent_activation='hard_sigmoid',
                                  activation=self.activation,
                                  kernel_initializer=self.init,
                                  return_sequences=False)(c5)

    #print(c5.shape)
    ## 3 up -> 128
    u4 = self.deconv_relu(64, name='Up_3_inp', pad='same')(c5)
    u4 = self.slice_and_concat(u4, c4)
    u4 = self.conv_relu(64, name='Up_3_out')(u4) # -> 64

    ## 2 up -> 64
    u3 = self.deconv_relu(32, name='Up_2_inp')(u4)
    u3 = self.slice_and_concat(u3, c3)
    u3 = self.conv_relu(32, 'Up_2_out')(u3) # -> 64

    ## 1 up -> 32
    u2 = self.deconv_relu(16, name='Up_1_inp')(u3)
    u2 = self.slice_and_concat(u2, c2)
    u2 = self.conv_relu(16, 'Up_1_out')(u2) # -> 16

    ## 0 up -> 16
    u1 = self.deconv_relu(8, name='Up_0_inp')(u2)
    u1 = self.slice_and_concat(u1, c1)
    u1 = self.conv_relu(8, 'Up_0_out')(u1) # -> 16

    ## output -> 8
    self.Outputs = keras.layers.Conv2D(filters=3, 
                                  kernel_size=(1,1),
                                  use_bias=True,
                                  padding='same',
                                  activation='softmax',
                                  name='output')(u1) # -> 3

    model = keras.Model(inputs=self.inputs, outputs=self.Outputs)
    
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    
    return model


if __name__ == "__main__":
  import numpy as np
  m = PhaseNet(3000, lstm=True)
  my_model = m.create_model()

  c_ = my_model.layers[1]
  #print(np.array(c_.get_weights()[0]).shape)
  my_model.summary()
  keras.utils.plot_model(my_model, show_shapes=True,show_layer_names=True)